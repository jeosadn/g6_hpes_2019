Referencias utiles:

Tensorflow en Nvidia:
https://www.pyimagesearch.com/2019/05/06/getting-started-with-the-nvidia-jetson-nano/

Keras tutorial:
https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5
https://github.com/fmezacr/patrones/blob/master/CNN/CNN_MNIST_KERAS.ipynb

Old tentative datasets:
https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia  -No usa camara
https://www.kaggle.com/grassknoted/asl-alphabet  -Mejor ~ 1GB
https://www.kaggle.com/mrgeislinger/asl-rgb-depth-fingerspelling-spelling-it-out -2 GB, no usamos depth

Chosen dataset:
https://www.kaggle.com/ucffool/dice-d4-d6-d8-d10-d12-d20-images

From the dataset, we have the training and validation sets:
./dice/train
./dice/valid

We will add our own images to the test dataset:
./test

Number of layers:
   The number of layers should be 1 or 2. More than thats produces a result that is too difficult to train. More info:
   https://stats.stackexchange.com/a/180052

Open questions:
   Should we do data augmentation?
   Number of layers?
   Layer should have 1 or 2 convolution steps?
   Batch size? smaller is faster, more random training, larger takes a long time to converge

